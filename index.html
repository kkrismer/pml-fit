<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Konstantin Krismer" />

<meta name="date" content="2015-02-21" />

<title>Practical Machine Learning - Course Project</title>

<script src="index_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.1/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="index_files/highlight/default.css"
      type="text/css" />
<script src="index_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Practical Machine Learning - Course Project</h1>
<h4 class="author"><em>Konstantin Krismer</em></h4>
<h4 class="date"><em>2015-02-21</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#configuartion"><span class="toc-section-number">1</span> Configuartion</a></li>
<li><a href="#preprocessing"><span class="toc-section-number">2</span> Preprocessing</a></li>
<li><a href="#feature-engineering"><span class="toc-section-number">3</span> Feature engineering</a></li>
<li><a href="#exploratory-data-analysis"><span class="toc-section-number">4</span> Exploratory data analysis</a><ul>
<li><a href="#distribution-of-feature-values"><span class="toc-section-number">4.1</span> Distribution of feature values</a></li>
<li><a href="#feature-value-vs-class-label"><span class="toc-section-number">4.2</span> Feature value vs class label</a></li>
<li><a href="#unsupervised-feature-clustering"><span class="toc-section-number">4.3</span> Unsupervised feature clustering</a></li>
</ul></li>
<li><a href="#train-model"><span class="toc-section-number">5</span> Train model</a></li>
<li><a href="#estimate-prediction-accuracy-on-test-set"><span class="toc-section-number">6</span> Estimate prediction accuracy on test set</a></li>
<li><a href="#predict"><span class="toc-section-number">7</span> Predict</a></li>
</ul>
</div>

<div id="configuartion" class="section level1">
<h1><span class="header-section-number">1</span> Configuartion</h1>
<p>Load libraries:</p>
<pre class="r"><code>knitr::opts_chunk$set(cache=TRUE, cache.lazy=FALSE, fig.width=10, fig.height=7, tidy=TRUE)
library(caret)</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;
## 
## The following object is masked from &#39;package:stats&#39;:
## 
##     filter
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(reshape2)
library(knitr)</code></pre>
<p>Load data:</p>
<pre class="r"><code>data &lt;- read.csv(&quot;data/pml-training.csv&quot;)
training.idx &lt;- createDataPartition(y = data$classe, p = 0.6, list = FALSE)
training.set &lt;- data[training.idx, ]
test.set &lt;- data[-training.idx, ]
unclassified &lt;- read.csv(&quot;data/pml-testing.csv&quot;)</code></pre>
</div>
<div id="preprocessing" class="section level1">
<h1><span class="header-section-number">2</span> Preprocessing</h1>
<p>Remove features with <code>NA</code> values:</p>
<pre class="r"><code>na.features.training &lt;- colnames(training.set)[apply(training.set, 2, function(x) sum(is.na(x))) &gt; 
    0]
na.features.unclassified &lt;- colnames(unclassified)[apply(unclassified, 2, function(x) sum(is.na(x))) &gt; 
    0]
na.features &lt;- union(na.features.training, na.features.unclassified)
training.set &lt;- training.set[, !(colnames(training.set) %in% na.features)]
unclassified &lt;- unclassified[, !(colnames(unclassified) %in% na.features)]
print(kable(data.frame(features = na.features), col.names = c(&quot;features with NA values&quot;), 
    caption = &quot;Removed the following NA-containing features:&quot;))</code></pre>
<pre><code>## 
## 
## Table: Removed the following NA-containing features:
## 
## features with NA values  
## -------------------------
## max_roll_belt            
## max_picth_belt           
## min_roll_belt            
## min_pitch_belt           
## amplitude_roll_belt      
## amplitude_pitch_belt     
## var_total_accel_belt     
## avg_roll_belt            
## stddev_roll_belt         
## var_roll_belt            
## avg_pitch_belt           
## stddev_pitch_belt        
## var_pitch_belt           
## avg_yaw_belt             
## stddev_yaw_belt          
## var_yaw_belt             
## var_accel_arm            
## avg_roll_arm             
## stddev_roll_arm          
## var_roll_arm             
## avg_pitch_arm            
## stddev_pitch_arm         
## var_pitch_arm            
## avg_yaw_arm              
## stddev_yaw_arm           
## var_yaw_arm              
## max_roll_arm             
## max_picth_arm            
## max_yaw_arm              
## min_roll_arm             
## min_pitch_arm            
## min_yaw_arm              
## amplitude_roll_arm       
## amplitude_pitch_arm      
## amplitude_yaw_arm        
## max_roll_dumbbell        
## max_picth_dumbbell       
## min_roll_dumbbell        
## min_pitch_dumbbell       
## amplitude_roll_dumbbell  
## amplitude_pitch_dumbbell 
## var_accel_dumbbell       
## avg_roll_dumbbell        
## stddev_roll_dumbbell     
## var_roll_dumbbell        
## avg_pitch_dumbbell       
## stddev_pitch_dumbbell    
## var_pitch_dumbbell       
## avg_yaw_dumbbell         
## stddev_yaw_dumbbell      
## var_yaw_dumbbell         
## max_roll_forearm         
## max_picth_forearm        
## min_roll_forearm         
## min_pitch_forearm        
## amplitude_roll_forearm   
## amplitude_pitch_forearm  
## var_accel_forearm        
## avg_roll_forearm         
## stddev_roll_forearm      
## var_roll_forearm         
## avg_pitch_forearm        
## stddev_pitch_forearm     
## var_pitch_forearm        
## avg_yaw_forearm          
## stddev_yaw_forearm       
## var_yaw_forearm          
## kurtosis_roll_belt       
## kurtosis_picth_belt      
## kurtosis_yaw_belt        
## skewness_roll_belt       
## skewness_roll_belt.1     
## skewness_yaw_belt        
## max_yaw_belt             
## min_yaw_belt             
## amplitude_yaw_belt       
## kurtosis_roll_arm        
## kurtosis_picth_arm       
## kurtosis_yaw_arm         
## skewness_roll_arm        
## skewness_pitch_arm       
## skewness_yaw_arm         
## kurtosis_roll_dumbbell   
## kurtosis_picth_dumbbell  
## kurtosis_yaw_dumbbell    
## skewness_roll_dumbbell   
## skewness_pitch_dumbbell  
## skewness_yaw_dumbbell    
## max_yaw_dumbbell         
## min_yaw_dumbbell         
## amplitude_yaw_dumbbell   
## kurtosis_roll_forearm    
## kurtosis_picth_forearm   
## kurtosis_yaw_forearm     
## skewness_roll_forearm    
## skewness_pitch_forearm   
## skewness_yaw_forearm     
## max_yaw_forearm          
## min_yaw_forearm          
## amplitude_yaw_forearm</code></pre>
<p>Remove features deemed insignificant (based on common sense and lack of association with target variable):</p>
<pre class="r"><code>conttable.user_name &lt;- table(training.set$user_name, training.set$classe)
print(conttable.user_name)</code></pre>
<pre><code>##           
##              A   B   C   D   E
##   adelmo   716 475 476 306 423
##   carlitos 508 419 287 287 367
##   charles  546 445 332 394 411
##   eurico   508 351 286 332 342
##   jeremy   696 290 370 315 330
##   pedro    374 299 303 296 292</code></pre>
<pre class="r"><code>chisq.test(conttable.user_name)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  conttable.user_name
## X-squared = 153.5589, df = 20, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code># user_name feature will be removed even though there is some association
# with the target variable, since prediction should not be based on this
# feature
conttable.new_window &lt;- table(training.set$new_window, training.set$classe)
conttable.new_window</code></pre>
<pre><code>##      
##          A    B    C    D    E
##   no  3283 2232 2015 1882 2113
##   yes   65   47   39   48   52</code></pre>
<pre class="r"><code>fisher.test(conttable.new_window)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  conttable.new_window
## p-value = 0.5426
## alternative hypothesis: two.sided</code></pre>
<pre class="r"><code>insignificant.features &lt;- c(&quot;X&quot;, &quot;user_name&quot;, &quot;raw_timestamp_part_1&quot;, &quot;raw_timestamp_part_2&quot;, 
    &quot;cvtd_timestamp&quot;, &quot;new_window&quot;, &quot;num_window&quot;)
training.set &lt;- training.set[, !(colnames(training.set) %in% insignificant.features)]
unclassified &lt;- unclassified[, !(colnames(unclassified) %in% insignificant.features)]</code></pre>
</div>
<div id="feature-engineering" class="section level1">
<h1><span class="header-section-number">3</span> Feature engineering</h1>
<p>Since the range of values is significantly different between features, each feature is normalized:</p>
<pre class="r"><code># exclude categorical target feature from normalization
training.set.features &lt;- as.matrix(training.set[, 1:(ncol(training.set) - 1)])

training.set.norm &lt;- data.frame(apply(training.set.features, 2, function(x) {
    feature.mean &lt;- mean(x)
    feature.sd &lt;- sd(x)
    return(apply(as.matrix(x), 1, function(y) {
        return((y - feature.mean)/feature.sd)
    }))
}))

training.set.norm$classe &lt;- training.set$classe</code></pre>
</div>
<div id="exploratory-data-analysis" class="section level1">
<h1><span class="header-section-number">4</span> Exploratory data analysis</h1>
<div id="distribution-of-feature-values" class="section level2">
<h2><span class="header-section-number">4.1</span> Distribution of feature values</h2>
<pre class="r"><code>ggplot(melt(training.set.norm), aes(value)) + geom_histogram(aes(y = ..density..), 
    colour = &quot;black&quot;, fill = &quot;skyblue&quot;) + geom_density() + facet_wrap(~variable, 
    scales = &quot;free&quot;, ncol = 4) + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())</code></pre>
<p><img src="index_files/figure-html/featureValueDist-1.png" title="" alt="" width="768" /></p>
</div>
<div id="feature-value-vs-class-label" class="section level2">
<h2><span class="header-section-number">4.2</span> Feature value vs class label</h2>
<pre class="r"><code>ggplot(melt(training.set.norm), aes(x = classe, y = value, colour = classe)) + 
    geom_point() + facet_wrap(~variable, scales = &quot;free&quot;, ncol = 4) + theme(axis.text = element_blank(), 
    axis.ticks = element_blank(), axis.title = element_blank())</code></pre>
<p><img src="index_files/figure-html/featureVsClassLabel-1.png" title="" alt="" width="768" /></p>
</div>
<div id="unsupervised-feature-clustering" class="section level2">
<h2><span class="header-section-number">4.3</span> Unsupervised feature clustering</h2>
<p><strong>Distance metric:</strong>: <span class="math">\(1 - \rho\)</span> (Pearson product-moment correlation coefficient)</p>
<pre class="r"><code>dendro &lt;- as.dendrogram(hclust(as.dist(1 - cor(training.set.features, method = &quot;pearson&quot;)), 
    method = &quot;complete&quot;))
par(cex = 0.8, mar = c(5, 8, 4, 1))
plot(dendro)</code></pre>
<p><img src="index_files/figure-html/featureDendrogram-1.png" title="" alt="" width="960" /></p>
</div>
</div>
<div id="train-model" class="section level1">
<h1><span class="header-section-number">5</span> Train model</h1>
<p>Prepare folds for cross-validation:</p>
<pre class="r"><code>cv &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 2)</code></pre>
<pre class="r"><code>training.set.norm &lt;- training.set.norm[sample.int(nrow(training.set.norm), size = 300, 
    replace = FALSE), ]
training.set.norm &lt;- training.set.norm[, c(1:10, ncol(training.set.norm))]
fit &lt;- train(classe ~ ., data = training.set.norm, method = &quot;rf&quot;, trControl = cv, 
    prox = TRUE)</code></pre>
<p>Prediction accurracy on training data (in sample error):</p>
<pre class="r"><code>prediction.training &lt;- predict(fit, training.set.norm)
tbl &lt;- table(prediction.training, training.set.norm$classe)
tbl</code></pre>
<pre><code>##                    
## prediction.training  A  B  C  D  E
##                   A 95  0  0  0  0
##                   B  0 58  0  0  0
##                   C  0  0 47  0  0
##                   D  0  0  0 45  0
##                   E  0  0  0  0 55</code></pre>
<p>Accuracy:</p>
<pre class="r"><code>sum(diag(nrow(tbl)) * tbl)/sum(tbl)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>Baseline accuracy:</p>
<pre class="r"><code>max(table(training.set.norm$classe))/sum(table(training.set.norm$classe))</code></pre>
<pre><code>## [1] 0.3166667</code></pre>
</div>
<div id="estimate-prediction-accuracy-on-test-set" class="section level1">
<h1><span class="header-section-number">6</span> Estimate prediction accuracy on test set</h1>
<p>Remove <code>NA</code>-containing features and insignificant features:</p>
<pre class="r"><code>test.set &lt;- test.set[, !(colnames(test.set) %in% na.features)]
test.set &lt;- test.set[, !(colnames(test.set) %in% insignificant.features)]</code></pre>
<p>Perform normalization of test set values based on mean and standard deviation of training set values:</p>
<pre class="r"><code># exclude categorical target feature from normalization
test.set.features &lt;- as.matrix(test.set[, 1:(ncol(test.set) - 1)])

# calculate mean and sd of training set features:
training.set.features &lt;- as.matrix(training.set[, 1:(ncol(training.set) - 1)])
feature.characteristics &lt;- data.frame(apply(training.set.features, 2, function(x) {
    return(c(mean(x), sd(x)))
}))

i &lt;- 1
test.set.norm &lt;- data.frame(apply(test.set.features, 2, function(x) {
    feature.mean &lt;- feature.characteristics[1, i]
    feature.sd &lt;- feature.characteristics[2, i]
    i &lt;&lt;- i + 1
    return(apply(as.matrix(x), 1, function(y) {
        return((y - feature.mean)/feature.sd)
    }))
}))
test.set.norm$classe &lt;- test.set$classe</code></pre>
<p>Prediction accurracy on test data (in sample error):</p>
<pre class="r"><code>prediction.test &lt;- predict(fit, test.set.norm)
tbl &lt;- table(prediction.test, test.set.norm$classe)
tbl</code></pre>
<pre><code>##                
## prediction.test    A    B    C    D    E
##               A 1844  282  310  371  235
##               B  111  950  189   84   36
##               C   80  139  706  170   15
##               D   99   18   88  580  152
##               E   98  129   75   81 1004</code></pre>
<p>Accuracy:</p>
<pre class="r"><code>sum(diag(nrow(tbl)) * tbl)/sum(tbl)</code></pre>
<pre><code>## [1] 0.6479735</code></pre>
<p>Baseline accuracy:</p>
<pre class="r"><code>max(table(test.set.norm$classe))/sum(table(test.set.norm$classe))</code></pre>
<pre><code>## [1] 0.2844762</code></pre>
</div>
<div id="predict" class="section level1">
<h1><span class="header-section-number">7</span> Predict</h1>
<p>Perform normalization of unclassified values based on mean and standard deviation of training set values:</p>
<pre class="r"><code># exclude problem id feature from normalization
unclassified.features &lt;- as.matrix(unclassified[, 1:(ncol(unclassified) - 1)])

i &lt;- 1
unclassified.norm &lt;- data.frame(apply(unclassified.features, 2, function(x) {
    feature.mean &lt;- feature.characteristics[1, i]
    feature.sd &lt;- feature.characteristics[2, i]
    i &lt;&lt;- i + 1
    return(apply(as.matrix(x), 1, function(y) {
        return((y - feature.mean)/feature.sd)
    }))
}))</code></pre>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
